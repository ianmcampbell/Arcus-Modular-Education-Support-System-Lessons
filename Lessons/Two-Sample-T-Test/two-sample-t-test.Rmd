---
title: "Two-Sample _t_ Test"
output: learnr::tutorial
runtime: shiny_prerendered
---

```{r setup, include=FALSE}
options(digits = 3, scipen = 9999)
if(!require(learnr)){install.packages("learnr")}
library(learnr)
this_Input =("
Group Value
2pm    69    
2pm    70    
2pm    66    
2pm    63    
2pm    68    
2pm    70    
2pm    69    
2pm    67    
2pm    62    
2pm    63    
2pm    76    
2pm    59    
2pm    62    
2pm    62    
2pm    75    
2pm    62    
2pm    72     
2pm    63    
5pm    68
5pm    62
5pm    67
5pm    68
5pm    69
5pm    67
5pm    61
5pm    59
5pm    62
5pm    61
5pm    69
5pm    66
5pm    62
5pm    62
5pm    61
5pm    70
")
ht = read.table(textConnection(this_Input),header=TRUE)
names(ht) <- c("section", "inches")
tt_height <- t.test(inches ~ section, 
                    data =ht,
                    var.equal = TRUE,
                    conf.level = 0.95)
p_height <- round(tt_height$p.value, 2)
```

## Background

There are several statistical tests that use the _t_ distribution ([What's a distribution?](https://www.khanacademy.org/math/statistics-probability/displaying-describing-data/comparing-features-distributions/v/comparing-distributions?modal=1)) and can be called _t_ tests. One is Student's _t_ test for two samples, named after "Student," the pseudonym William Gosset used to hide his employment by the Guinness brewery in the early 1900s (they had a rule that their employees weren't allowed to publish, and Guinness didn't want other employees to know that they were making an exception for Gosset). Student's _t_ test for two samples compares the means in two groups.

```{r aa, echo=FALSE}
question("What might have been one of Student's favorite pasttimes?",
      answer("Drinking beer", correct = TRUE),
      answer("Working as a hairdresser"),
      answer("Flying to Jupiter's moons"),
      answer("Going to school full time")
)
```

### Two-sample _t_ Test Variables

>* One [scalar (a.k.a. "measurement") variable](http://www.biostathandbook.com/variabletypes.html#measurement)     
* One [nominal variable](http://www.biostathandbook.com/variabletypes.html) with only two categories or two categories that you have selected. You can also use a binary variable here as it is functionally a nominal variable with two categories. 

Student's _t_ test for two samples shows whether the means of the measurement variable are significantly different in two groups. 

It is mathematically identical to a one-way ANOVA with two categories; but because comparing the means of two samples is such a common experimental design, and because the _t_ test is familiar to many more people than the ANOVA, we treat the two-sample _t_ test separately.

```{r a, echo=FALSE}
question("What other test does the two-sample _t_ test NOT resemble?",
         answer("a one-sample _t_ test"),
         answer("a one-way ANOVA with only two categories"),
         answer("a paired _t_ test"),
         answer("a linear regression model", correct = TRUE)
)
```


### Null and Alternative Hypotheses

>* **H<sub>0</sub>**: The means of the measurement variable are equal in the two groups.     
* **H<sub>A</sub>** (2-sided): The means of the measurement variable are not equal in the two groups.     
* **H<sub>A</sub>** (1-sided): The mean of the measurement variable is higher (specifically---or lower specifically, but not either-or because that would be a 2-sided test) in one of the groups.

```{r bb, echo=FALSE}
question("What is NOT an example of a 1-sided null hypothesis?",
         answer("It usually takes less than one hour to make pie dough"),
         answer("The average monthly electric bill in Philadelphia, PA is $92", correct = TRUE),
         answer("The average monthly electric bill in Philadelphia, PA is less than $100"),
         answer("The average monthly electric bill in Philadelphia, PA is more than $100")
)
```

### Examples of the Null and Alternative Hypotheses

You have become curious about the sizes of adult children's feet. Are right feet the same as left feet?

There are two variables in that question. 

```{r two-vars, echo=FALSE}
question("What are the two variables?",
         answer("right feet and left feet"),
         answer("foot size and type of person (adult vs. child)"),
         answer("foot size and foot type (left vs. right)", correct = TRUE)
)
```

In fall 2004, students in the 2 p.m. section of John McDonald's Biological Data Analysis class had an average height of 66.6 inches, while the average height in the 5 p.m. section was 64.6 inches. Are the average heights of the two sections significantly different? 

First, let's look at the data. It's been loaded in `ht` for you. Use `head` to look at it, then change the variable names. There are lots of ways to change variable names in R. In this case, try simply stating `names(ht) <- c("section", "inches")`. Then type `names(ht)` to see if it worked. 

```{r heightnames, exercise = TRUE}

```
```{r heightnames-solution}
head(ht)
names(ht) <- c("section", "inches")
names(ht)
```

The null hypothesis and alternatives are, therefore

>* **Example H<sub>0</sub>**: "Mean heights in the two sections are the same."     
* **Example H<sub>A</sub>** (2-sided): "Mean heights in the two sections are not the same."     
* **Example H<sub>A</sub>** (1-sided, version 1): "Mean height in the 2 pm section is higher than in the 5 pm section."     
* **Example H<sub>A</sub>** (1-sides, version 2): "Mean height in the 5 pm section is higher than in the 2 pm section."     

We'll use the aptly named function `t.test` to see if the means are NOT the same in the two sections. Pass it the formula `inches ~ section`, then `data = ` the data set name, then two parameters: `alternative` set to `two.sided` and `conf.level` set to the usual 0.95.


```{r height-t-test, exercise = TRUE}

```
```{r height-t-test-solution}
t.test(inches ~ section, # the nominal variable with two options
       data =ht,    # the measurement or scalar variable
       alternative = "two.sided", 
       conf.level = 0.95)
```

### Interpretation

According to the output, _p_ is `r p_height`. We would reject the null hypothesis if _p_ < .05 (the usual standard [but don't get me started]). In this case, it's not. We cannot reject the null hypothesis that mean heights in the two groups are the same. 

>Note: We have not proven that the null hypothesis is true, just that we can't say it's _not_ true. 

To report the results of a two-sample _t_ test, you would provide the test statistic (_t_), the [degrees of freedom](https://en.wikipedia.org/wiki/Degrees_of_freedom_(statistics)) (df), and the _p_ value (_p_) in a sentence like this: "based on the results of a _t_ test (_t_ = `r round(tt_height$statistic[[1]], 2)`), df = `r round(tt_height$parameter[[1]], 2)`, _p_ = `r round(tt_height$p.value[[1]], 2)`, we do not reject the null hypothesis that the two group means are the same." To be truly thorough, provide also the [confidence interval](https://www.khanacademy.org/math/statistics-probability/displaying-describing-data/comparing-features-distributions/v/comparing-distributions?modal=1)

## More about the Output

Run `t.test` again on the same data, but this time store the result in `tt_height`. 

```{r prepare-height_ttest}
options(digits = 3, scipen = 9999)
if(!require(learnr)){install.packages("learnr")}
library(learnr)
Input =("
Group Value
2pm    69    
2pm    70    
2pm    66    
2pm    63    
2pm    68    
2pm    70    
2pm    69    
2pm    67    
2pm    62    
2pm    63    
2pm    76    
2pm    59    
2pm    62    
2pm    62    
2pm    75    
2pm    62    
2pm    72     
2pm    63    
5pm    68
5pm    62
5pm    67
5pm    68
5pm    69
5pm    67
5pm    61
5pm    59
5pm    62
5pm    61
5pm    69
5pm    66
5pm    62
5pm    62
5pm    61
5pm    70
")
height = read.table(textConnection(Input),header=TRUE)
names(ht) <- c("section", "inches")
tt_height <- t.test(inches ~ section, 
                    data =ht,
                    var.equal = TRUE,
                    conf.level = 0.95)
p_height <- round(tt_height$p.value, 2)
```
```{r heigh_ttest, exercise=TRUE}

```
```{r heigh_ttest-solution}
tt_height <- t.test(inches ~ section, # the nominal variable with two options
                    data =ht,    # the measurement or scalar variable
                    alternative = "two.sided", 
                    conf.level = 0.95)
```

In R, everything is an object. Use `str` (short for "structure"--and so many other things, but here all we care about is "structure") to find out what the output looks like to R. Pass `str` the object you just created.

```{r object_appearance, exercise = TRUE, exercise.setup = "prepare-height_ttest"}

```
```{r object_appearance-solution}
str(tt_height)
```

That's interesting. What looked like a paragraph of output was actually a list! Try accessing elements of the list, for instance, `method`.

```{r access_method, exercise = TRUE, exercise.setup = "prepare-height_ttest"}

```
```{r access_method-solution}
tt_height$method
```

You can use output very nicely in your [reproducible Markdown text](https://education.arcus.chop.edu/scripted-analysis/) by calling it within text while you write a Markdown document. For instance, you might write, "I just did a " and then type a tic-mark ("\`", likely your lowercase "~", in the upper left corner of your keyboard, not to be confused with a single quote), then the text "r tt_height$method" without quotation marks, and finally, a closing tic-mark. The rendered (knitted) output would be 

>I just did a `r tt_height$method`. 

Let's go through the _t_ test outputted ("put out"?) list one by one.

### Statistic

Access `statistic` within `tt_height`.

```{r access_statistic, exercise = TRUE, exercise.setup = "prepare-height_ttest"}

```
```{r access_statistic-solution}
tt_height$statistic
```



```{r statistic_output, echo=FALSE}
question("What is the value of statistic?",
         answer("66.55556"),
         answer("1.29", correct = TRUE),
         answer("31.17529"),
         answer("0")
)
```



```{r statistic_name, echo=FALSE}
question("What's it called?",
         answer("_p_"),
         answer("df"),
         answer("confidence interval"),
         answer("_t_", correct = TRUE)
)
```

### Parameter

Access `parameter` within `tt_height`.

```{r access_parameter, exercise = TRUE, exercise.setup = "prepare-height_ttest"}

```
```{r access_parameter-solution}
tt_height$parameter
```

```{r parameter_output, echo=FALSE}
question("What is the parameter's value?",
         answer("66.55556"),
         answer("1.310886"),
         answer("32", correct = TRUE),
         answer("0")
)
```

```{r parameter_name, echo=FALSE}
question("What's it called?",
         answer("_p_"),
         answer("df", correct = TRUE),
         answer("confidence interval"),
         answer("_t_")
)
```


### `p.value`

Access `p.value` within `tt_height`.

```{r access_p, exercise = TRUE, exercise.setup = "prepare-height_ttest"}

```
```{r access_p-solution}
tt_height$p.value
```

```{r p_output, echo=FALSE}
question("What is _p_'s value?",
         answer("66.55556"),
         answer("1.310886"),
         answer("31.2"),
         answer(".20", correct = TRUE)
)
```

```{r p_name, echo=FALSE}
question("What's it called?",
         answer("_p_", correct = TRUE),
         answer("df"),
         answer("confidence interval"),
         answer("_t_")
)
```

### Confidence Interval

([What's a confidence interval?](https://www.khanacademy.org/math/statistics-probability/confidence-intervals-one-sample/introduction-to-confidence-intervals/v/confidence-intervals-and-margin-of-error?modal=1))

Access `conf.int` within `tt_height`.

```{r access_conf, exercise = TRUE, exercise.setup = "prepare-height_ttest"}

```
```{r access_conf-solution}
tt_height$conf.int
```

```{r conf_output, echo=FALSE}
question("What is the confidence interval?",
         answer("0.95"),
         answer("[-1.12, 4.98]", correct = TRUE),
         answer("31.2"),
         answer("0.199")
)
```

## Assumptions

Every statistical test relies upon assumptions about the data you give it. The next few sections are about the assumptions upon which a two-sample _t_ test rests. Note that the _t_ test is fairly reliable even when assumptions are violated, within reason. 

### Normality

The _t_ test assumes that the observations _within each group_ (as opposed to all of the observations together before you broke them into groups) are normally distributed. 

Fortunately, it is not at all sensitive to deviations from this assumption if the distributions of the two groups are the same (if both distributions are skewed to the right, for example). 

[John McDonald wrote](http://www.biostathandbook.com/twosamplettest.html) that he had done simulations with a variety of non-normal distributions, including flat, bimodal, and highly skewed, and the two-sample _t_ test always gave about 5% false positives, even with very small sample sizes. If your data is severely non-normal, you should try to find a data transformation that makes it more normal; but don't worry if you can't find a good transformation or don't have enough data to check the normality.

```{r bad_distributions, echo=FALSE}
question("Which of the following situations causes a two-sample _t_ test to give too many false positives?",
         answer("A very small sample size"),
         answer("None of these choices (except this one)", correct = TRUE),
         answer("A flat distribution"),
         answer("A bimodal distribution")
)
```

If your data is severely non-normal, _and_ you have different distributions in the two groups (one data set is skewed to the right and the other is skewed to the left, for example), _and_ you have small samples (less than 50 or so), then the two-sample _t_ test can give inaccurate results, with considerably more than 5% false positives. A data transformation won't help you here, and neither will a Mann-Whitney U-test. It would be pretty unusual in biology to have two groups with different distributions but equal means, but if you think that's a possibility, you should require a _p_ value much less than 0.05 to reject the null hypothesis.

```{r tt_perfect_storm, echo=FALSE}
question("Which of the following is NOT a necessary aspect of a distribution that will cause a _t_ test to result in too many false positives?",
         answer("Small sample size"),
         answer("Non-normality"),
         answer("Two groups with skewness in opposite directions"),
         answer("Both negative and positive numbers", correct = TRUE)
)
```

```{r tt_tf_assumptions, echo=FALSE}
question("True or false: You can still perform a reliable _t_ test if only two out of three of the conditions in the previous question have been met.",
         answer("TRUE", correct = TRUE),
         answer("FALSE")
)
```

### Homoscedasticity

In addition to normality, the two-sample _t_ test also assumes homoscedasticity (equal variances in the two groups). If you have a balanced design (equal sample sizes in the two groups), the test is not very sensitive to heteroscedasticity unless the sample size is very small (less than 10 or so); the standard deviations in one group can be several times as big as in the other group, and you'll get _p_ < 0.05 about 5% of the time if the null hypothesis is true. 

With an unbalanced design, heteroscedasticity is a bigger problem; if the group with the smaller sample size has a bigger standard deviation, the two-sample _t_ test can give you false positives much too often. If your two groups have standard deviations that are substantially different (such as one standard deviation is twice as big as the other) and your sample sizes are small (less than 10) or unequal, you should use Welch's _t_ test instead.

```{r unbalanced, echo=FALSE}
question("What can cause a two-sample _t_ test to become more sensitive to the assumption of homoscedasticity?",
         answer("Heteroscedasticity"),
         answer("A very small sample (< 10 or so) and unequal sample sizes in the two groups", correct = TRUE),
         answer("Skewness in the data"),
         answer("Unequal sample sizes in the two groups")
)
```

```{r belch_for_welches, echo=FALSE}
question("What should you do if your two groups have standard deviations that are substantially different AND your sample sizes are small OR unequal",
         answer("Drink Welch's grape juice"),
         answer("Perform a squelch test instead of a _t_ test"),
         answer("Use linear regression instead of a _t_ test"),
         answer("Perform a Welch's _t_ test instead of the regular two-sample _t_ test", correct = TRUE)
)
```

### Test the Assumptions
 
We will use a histogram with an imposed normal curve to confirm data are approximately normal. First, though, let's just look at the data. We'll use functions from the `psych` package, so be sure to call `library(psych)` before calling the functions `headTail`, `str`, and `summary`, passing `ht$inches` to each of them. (Tip: Sometimes if I know I'll be passing data to several functions, I'll set it equal to a shorter name, here `xht`, so I can just type `xht` instead of the longer name.)

```{r f, exercise = TRUE}

```
```{r f-solution}
xht <- ht$inches
library(psych)
headTail(xht)
str(xht)
summary(xht)
```

Use the function `plotNormalHistogram` from the `rcompanion` package to see if the data is approximately normally distributed. All the function requires is the variable with the sodium levels, `Sodium`. Make sure to put `rcompanion` into the library before calling `plotNormalHistogram`.

```{r g, exercise = TRUE}

```
```{r g-solution}
library(rcompanion)
plotNormalHistogram(xht)
```

The shape is not normal. Let's look at a normal quantile plot of the data to find out more. You don't have to store `ht$inches` in the variable `xht`, but I did. Then pass `xht` to `qqnorm`. Evaluate the resulting plot. It needs a red line, so call `qqline` and pass it `xht` and tell it to use `col` equal to "red".

```{r h, exercise = TRUE}

```
```{r h-solution}
qqnorm(xht)
qqline(xht, col = "red")
```

A perfectly normal distribution would have the dots exactly along the line. This is not perfectly normal, but it shows that the wonky histogram may have been due to the small sample size. There is a seemingly big difference between 2 people who are 65" tall and 6 people who are 66" tall, but in fact it's not all that damaging. 

We'll say the example passes assumption tests for a two-sample _t_ test.

## Effect Size: How much of a Difference do we Care About?


## Power Analysis for a Two-Sample _t_ Test

To estimate the sample sizes needed to detect a significant difference between two means, you need the following:

* **the effect size**, or the difference in means you hope to detect
* **the standard deviation**. Usually you'll use the same standard deviation value for each group, but if you know ahead of time that one group will have a larger standard deviation than the other, you can use different numbers
* **alpha**, or the significance level (usually 0.05)
* **beta**, the probability of accepting the null hypothesis when it is false (0.50, 0.80 and 0.90 are common values)
* **the ratio of one sample size to the other**. The most powerful design is to have equal numbers in each group (N1/N2=1.0), but sometimes it's easier to get large numbers of one of the groups. For example, if you're comparing the bone strength in mice that have been reared in zero gravity aboard the International Space Station vs. control mice reared on earth, you might decide ahead of time to use three control mice for every one expensive space mouse (N1/N2=3.0)

The [G\*Power program](http://www.psycho.uni-duesseldorf.de/abteilungen/aap/gpower3/) will calculate the sample size needed for a two-sample t–test. Choose "t tests" from the "Test family" menu and "Means: Difference between two independent means (two groups" from the "Statistical test" menu. Click on the "Determine" button and enter the means and standard deviations you expect for each group. Only the difference between the group means is important; it is your effect size. Click on "Calculate and transfer to main window". Change "tails" to two, set your alpha (this will almost always be 0.05) and your power (0.5, 0.8, or 0.9 are commonly used). If you plan to have more observations in one group than in the other, you can make the "Allocation ratio" different from 1.
As an example, let's say you want to know whether people who run regularly have wider feet than people who don't run. You look for previously published data on foot width and find the ANSUR data set, which shows a mean foot width for American men of 100.6 mm and a standard deviation of 5.26 mm. You decide that you'd like to be able to detect a difference of 3 mm in mean foot width between runners and non-runners. Using G\*Power, you enter 100 mm for the mean of group 1, 103 for the mean of group 2, and 5.26 for the standard deviation of each group. You decide you want to detect a difference of 3 mm, at the P<0.05 level, with a probability of detecting a difference this large, if it exists, of 90% (1−beta=0.90). Entering all these numbers in G*Power gives a sample size for each group of 66 people.

```{r o, exercise = TRUE}

```
```{r o-solution}
library(lsr)
cohensD(sodium$Sodium, 1500)
```

Cohens _d_ for `sodium$Sodium` is 1.096864. What does that mean?

### Interpreting Cohen's _d_

Cohen's _d_ ranges from 0 to $\normalsize \infty$, with 0 indicating no effect: the mean equals $\normalsize \mu$.  Cohen’s d can be positive or negative depending on whether the mean is greater than or less than mu. Our sodium effect size is 1.096864 standard deviations from the mean.

#### Relating Cohen's _d_ to the Difference between the Theoretical and Sample Means

What's the standard deviation of `sodium$Sodium`?

```{r p, exercise = TRUE}

```
```{r p-solution}
sd(sodium$Sodium)
```
 
So the actual effect size we found in the _t_ test about sodium levels in students is `1.096864 * sd(sodium$Sodium)`. Calculate that and store it in the variable `sodium_effect`. Feel free to store `sodium$Sodium` in `xNa` so you can type it just once. Then print out the difference between the theoretical mean and the sample mean. 

```{r q, exercise = TRUE}

```
```{r q-solution}
xNa <- sodium$Sodium
(sodium_effect <- 1.096864 * sd(xNa))
1500 - mean(xNa)
```

By comparing the two values, we basically just checked our work and found it to be correct.

#### Relating Cohen's _d_ to Effect Size in the World at Large

But forget about the actual change in sodium. We used Cohen's _d_ to _standardize_ the effect size so we could compare the importance of the differences we found between sample words per minute (55.3125) and expected words per minute (40) and that between sample sodium intake (1287.5) grams and expected sodium intake (1500 grams). Which are the relative effect sizes?
 
A Cohen’s d of 0.5 suggests that the mean and $\normalsize \mu$ differ by one-half the standard deviation of the data. A Cohen’s _d_ of 1.0 suggests that the mean and $\normalsize \mu$ differ by one standard deviation of the data.

Rule of thumb about effect sizes: 

* Small effect = 0.2    
* Medium effect = 0.5    
* Large effect = 0.8    

To review, calculate the two Cohen's _d_'s for the two problems we worked above. Would you call them small, medium, or large effects?

```{r r, exercise = TRUE}

```
```{r r-solution}
cohensD(wpm$words_per_minute, muwpm)
cohensD(sodium$Sodium, muNa)
```

Both the effects we found are large (> .8)

## Graphing the Data against the Default (i.e. Expected) Value

Here is a box plot with the default value highlighted. This next code chunk isn't an exercise. It's an example of code you can use to report your findings. 

```{r s}
library(ggplot2)

# ggplot(data=sodium, 
#        aes(x = Instructor, y = Sodium)) +
#        geom_boxplot() +
#        geom_point(aes(x = 1, y = 1500), 
#                   colour="blue",
#                   size = 8,
#                   shape = "+") +
#        theme_bw() +
#        theme(axis.title = element_text(face = "bold")) +
#        theme(axis.text = element_text(face = "bold"))

```

Now use that code to create another box plot, this one for the words-per-minute data. Go ahead and improve on the plot design by adding a label for the y axis that isn't the variable name but rather the variable _label_. If you don't know how to do this, you can click on the Solution button and peek or you can do an internet search for "adding labels to my ggplot". There are several ways to accomplish this. And try changing the theme to minimal to see if you prefer the output.

```{r t, exercise = TRUE}

```
```{r t-solution}
# library(ggplot2)
# 
# ggplot(data=wpm, 
#        aes(x = Instructor, y = words_per_minute)) +
#        geom_boxplot() +
#        geom_point(aes(x = 1, y = 40), 
#                   colour="blue",
#                   size = 8,
#                   shape = "+") +
#        theme_minimal() +
#        theme(axis.title = element_text(face = "bold")) +
#        theme(axis.text = element_text(face = "bold")) +
#   labs(y = "Words per Minute")
```

## Teach me like I like Math

![Here, Sal performs a _t_ test by hand](https://www.khanacademy.org/math/ap-statistics/two-sample-inference/two-sample-t-test-means/v/two-sample-t-test-for-difference-of-means)

![Here, Sal talks about confidence intervals and _t_ tests](https://www.khanacademy.org/math/statistics-probability/confidence-intervals-one-sample/old-confidence-interval-videos/v/t-statistic-confidence-interval)

## References

This lesson is heavily based with thanks on the works of John H. McDonald ([Handbook of Biological Statistics](http://www.biostathandbook.com/chigof.html)) and Salvatore S. Mangiafico ([R Companion to the Biostats Handbook](https://rcompanion.org/rcompanion/b_03.html)).
