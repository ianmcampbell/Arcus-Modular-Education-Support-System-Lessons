---
title: "Chi-Square Test of Goodness-of-Fit"
output: learnr::tutorial
runtime: shiny_prerendered
---

```{r setup, include=FALSE}
library(learnr)
if(!require(rcompanion)){install.packages("rcompanion")}
Input =("
Value     Douglas.fir  Ponderosa.pine  Grand.fir   Western.larch
Observed  0.4487179    0.5064103       0.01923077  0.02564103
Expected  0.5400000    0.4000000       0.05000000  0.01000000   
")

trees = as.matrix(read.table(textConnection(Input),
                   header=TRUE, 
                   row.names=1))

observed_trees <- c(70, 79, 3, 4)
expected_trees <- c(0.54, 0.40, 0.05, 0.01)
total <- sum(observed_trees)
observed_trees_prop <- observed_trees / total

Input =("
Tree              Value      Count   Total Proportion  Expected
'Douglas fir'     Observed   70      156   0.4487      0.54
'Douglas fir'     Expected   54      100   0.54        0.54
'Ponderosa pine'  Observed   79      156   0.5064      0.40
'Ponderosa pine'  Expected   40      100   0.40        0.40
'Grand fir'       Observed    3      156   0.0192      0.05
'Grand fir'       Expected    5      100   0.05        0.05
'Western larch'   Observed    4      156   0.0256      0.01
'Western larch'   Expected    1      100   0.01        0.01
")
  
forage = read.table(textConnection(Input),header=TRUE)
library(dplyr)
forage <- mutate(forage,
                  Tree = factor(Tree, levels = unique(Tree)),
                  Value = factor(Value, levels = unique(Value))
                  )
forage = 
mutate(forage,       
       low.ci = apply(forage[c("Count", "Total", "Expected")], 
                       1, 
                       function(x) 
                       binom.test(x["Count"], x["Total"], x["Expected"]
                                 )$conf.int[1]),
                                 
        upper.ci = apply(forage[c("Count", "Total", "Expected")],
                         1,
                         function(x) 
                         binom.test(x["Count"], x["Total"], x["Expected"]
                                    )$conf.int[2])
         )

forage$low.ci[forage$ Value == "Expected"] = 0
forage$upper.ci[forage$ Value == "Expected"] = 0

```

## Background

This section is based with thanks on the works of John H. McDonald ([Handbook of Biological Statistics](http://www.biostathandbook.com/chigof.html)) and Salvatore S. Mangiafico ([R Companion to the Biostats Handbook](https://rcompanion.org/rcompanion/b_03.html)).

### Variables

>One [nominal variable](http://www.biostathandbook.com/variabletypes.html#nominal)    

### Null and Alternative Hypotheses

> **H<sub>0</sub>**: The nominal variable of interest's cateogory (a.k.a. "level") proportions _are not different_ from theoretical proportions.       
**H<sub>A</sub>** (which is 2-sided): The nominal variable of interest's category proportions _are different_ from theoretical proportions.         

### Examples of the Null and Alternative Hypotheses

>**Example H<sub>0</sub>**: "The number of times a flipped coin lands on its head *is not different* from 50% of the total number of tosses."    
**Example H<sub>A</sub>** (2-sided): "The number of times a flipped coin lands on its head *is different* from 50% of the total number of tosses."     

The null hypothesis is usually an extrinsic hypothesis, which means you knew the expected proportions before doing the experiment. Examples include a 1:1 heads to tails ratio or a 1:2:1 ratio in a genetic cross. Another example would be looking at an area of shore that had 59% of the area covered in sand, 28% mud, and 13% rocks; if you were investigating where seagulls like to stand, your null hypothesis would be that 59% of the seagulls were standing on sand, 28% on mud, and 13% on rocks.

Significant results can be reported as “The nominal variable's category proportions were statistically different from theoretical proportions.”

```{r vars, echo=FALSE}
question("You might use a chi-square goodness-of-fit test to examine which of the following relationships?",
      answer("The number of people from various countries represented at a conference in comparison with the proportion of people from those countries expected to be represented at the conference", correct = TRUE),
      answer("Number of people from two countries"),
      answer("The relationships among age groups"),
      answer("Height, weight, and season")
)
```

### Why use it? 

Use a chi-square test to compare a categorical (a.k.a. "nominal") variable's category proportions to theoretical proportions. People are usually familiar with them, so if you have enough members in each of your nominal variable's categories, this test is the best choice. 

There are alternatives. Common goodness-of-fit tests are 

* chi-square    
* G-test    
* binomial or multinomial exact tests
 
In general, there are no assumptions about the distribution of data for these tests. However, the results of chi-square tests and G-tests can be inaccurate if level or cell counts are low. A rule of thumb is that all statistically expected cell counts should be 5 or greater for chi-square and G-tests. If you have fewer than that in any level, use exact tests, which are not bothered by low cell counts.

The advantage of chi-square tests is that your audience may be more familiar with them.
 
G-tests are also called "likelihood ratio tests" or, by SAS, a “Likelihood Ratio Chi-Square”.

```{r a-1, echo=FALSE}
question("When would you NOT use a chi-square goodness-of-fit test?",
      answer("When you can't prove that your data is normally distributed"),
      answer("When your data is very large"),
      answer("When the count per category is less than 5", correct = TRUE),
      answer("Now, because there is always a quiz")
)
```

### How it Works

Unlike the [exact test of goodness-of-fit](http://www.biostathandbook.com/exactgof.html), when you do a chi-square test you do not directly calculate the probability of obtaining the observed results or something more extreme. Instead, like almost all statistical tests, the chi-square test has an intermediate step: it uses the data to calculate a test statistic that measures how far the observed data is from the null _expectation_. You then use a mathematical relationship, in this case the chi-square distribution, to estimate the probability of obtaining that value of the test statistic (i.e., that value for chi squared).

You calculate chi squared by taking the cateogory's observed count (O for "Observed"), subtracting its theoretical or "expected" count (E), then squaring this difference. The larger the deviation from the null hypothesis, the larger is the difference between observed and expected. You square the differences to make them all positive. 

```{r a-2, echo=FALSE}
question("You square the difference between what and what when you calculate a chi-square test statistic?",
         answer("The first category and the second category, then the first and the third categories, and so on"),
         answer("What you hoped to have for dinner and what was actually on offer"),
         answer("The value of a scalar variable for one group and for another group"),
         answer("The observed and the expected numbers", correct = TRUE)
)
```


The next step is to divide each difference by the expected number; then you add up these "standardized differences". 

The test statistic is approximately equal to the log-likelihood ratio used in the G–test. It is conventionally called a "chi-square" statistic, although this is somewhat confusing because it's just one of many test statistics that follows the theoretical chi-square distribution. The equation is

$\displaystyle \chi^{2} = ∑\frac{(O−E)^{2}}{E}$.

As with most test statistics, the larger the difference between observed and expected, the larger the test statistic becomes. 

To give an example, let's say your null hypothesis is a 3:1 ratio of smooth wings to wrinkled wings in offspring from a bunch of Drosophila crosses. You observe 770 flies with smooth wings and 230 flies with wrinkled wings; the expected values are 750 smooth-winged and 250 wrinkled-winged flies. Entering these numbers into the equation, the chi-square value is 2.13. If you had observed 760 smooth-winged flies and 240 wrinkled-wing flies, which is closer to the null hypothesis, your chi-square value would have been smaller, at 0.53; if you'd observed 800 smooth-winged and 200 wrinkled-wing flies, which is further from the null hypothesis, your chi-square value would have been 13.33.

```{r b, echo=FALSE}
question("Which leads to a rejected null hypothesis for a chi-square calculation?",
         answer("A large difference between the observed and expected values", correct = TRUE),
         answer("A small or no difference between the observed and expected values"),
         answer("A large number of categories in the variable being examined"),
         answer("A low value for p")
)
```

The distribution of the test statistic under the null hypothesis is approximately the same as the theoretical chi-square distribution. This means that once you know the chi-square value and the number of degrees of freedom, you can calculate the probability of getting that value of chi-square using the chi-square distribution. The number of degrees of freedom is the number of categories minus one, so for our example there is one degree of freedom.

The shape of the chi-square distribution depends on the number of degrees of freedom. For an extrinsic null hypothesis (the much more common situation, where you know the proportions predicted by the null hypothesis before collecting the data), the number of degrees of freedom is simply the number of values of the variable, minus one. Thus if you are testing a null hypothesis of a 1:1 flipped coin ratio, there are two possible values (heads and tails), and therefore one degree of freedom. This is because once you know how many of the total are heads (a number which is "free" to vary from 0 to the sample size), the number of tails is determined. If there are three values of the variable (such as red, pink, and white), there are two degrees of freedom, and so on.

```{r c, echo=FALSE}
question("For an extrinsic null hypothesis analysis, how many degrees of freedom are there when you expect 200 smooth-winged and 50 wrinkled-winged flies?",
         answer("199 + 49 - 1 = 247 because for an extrinsic null, you want n - 1 as the degrees of freedom"),
         answer("2 - 1 = 1 because for an extrinsic null, you want n - 1 (groups) as the degrees of freedom", correct = TRUE),
         answer("199 + 49 - 2 groups = 246 because for an extrinsic null, you want n - 1 (per group) as the degrees of freedom"),
         answer("You can't measure freedom")
)
```

An intrinsic null hypothesis is one where you estimate one or more parameters _from the data_ in order to get the numbers for your null hypothesis. One example is [Hardy-Weinberg proportions](https://www.khanacademy.org/science/biology/her/heredity-and-genetics/v/hardy-weinberg), which has to do with, for instance, [allele frequency](https://www.khanacademy.org/science/biology/her/heredity-and-genetics/v/allele-frequency). For an intrinsic null hypothesis, the number of degrees of freedom is calculated by taking the number of values of the variable, subtracting 1 for each parameter estimated from the data, then subtracting 1 more. 

Thus for Hardy-Weinberg proportions with two alleles and three genotypes, there are three values of the variable (the three genotypes); you subtract one for the parameter estimated from the data (the allele frequency, p); and then you subtract one more, yielding one degree of freedom. There are other statistical issues involved in testing fit to Hardy-Weinberg expectations, so if you need to do this, see Engels (2009) and the older references he cites.

## Practice

The following code chunk checks to see if you have the required packages and, if you don't, installs them to your R environment. You can copy and paste this code into your local copy of RStudio or run it from here to see what happens. 

```{r packages, exercise=TRUE}
if(!require(EMT)){install.packages("EMT")}
if(!require(DescTools)){install.packages("DescTools")}
if(!require(ggplot2)){install.packages("ggplot2")}
if(!require(rcompanion)){install.packages("rcompanion")}
```

#### Drosophila Example

Try the chi-square goodness-of-fit code using the function `chisq.test`. Let's use McDonald's Drosophila example. Since we need only counts and not data points, we can run the command without having first to load a data set. Here is the information you need:

* We expect a proportion of 3:1 of smooth-winged to wrinkle-winged Drosophila
* The sample has 770 smooth-winged flies 
* The sample has 230 wrinkle-winged flies

Please edit the following code to get a result:

```{r chisq-test, exercise = TRUE}
observed <- 1        # REPLACE WITH CORRECT VALUES
expected <- 1        # REPLACE WITH CORRECT VALUES
chisq.test(x = observed,
           p = expected)
```
```{r chisq-test-solution}
observed <- c(770, 230)        # observed frequencies
expected <- c(0.75, 0.25)      # expected proportions
chisq.test(x = observed,
           p = expected)
```

### Extrinsic Hypothesis for Male Red Crossbills

European crossbills (_Loxia curvirostra_) have the tip of the upper bill either right or left of the lower bill. Their crossed bills help them extract seeds from pine cones. Some researchers have hypothesized that frequency-dependent selection would keep the number of right and left-billed birds at a 1:1 ratio. Groth (1992) observed 1752 right-billed and 1895 left-billed crossbills.

Calculate the expected frequency of right-billed birds by multiplying the total sample size (3647) by the expected proportion (0.5) to yield 1823.5. Do the same for left-billed birds. The number of degrees of freedom when an for an extrinsic hypothesis is the number of classes minus one. In this case, there are two classes (right and left), so there is one degree of freedom.

```{r red-cross-bills, exercise = TRUE}

```
```{r red-cross-bills-solution}
#'
#' You can do this many ways. Here is one wordier example that arrives at the 
#' correct output. Sometimes you might want to make your code more wordy
#' if you have few concerns about memory or about computation time and if 
#' it is very important that the code be readable.

# Set up values
obs_right <- 1752 #right-crossed
obs_left <- 1895  #left crossed
observed <- c(obs_right, obs_left)
expected <- c(.5, .5)

# Use the values to perform the test
chisq.test(x = c(obs_right, obs_left),
           p = c(.5, .5))
```

You can reject the null hypothesis that there is no difference between observed and expected: there is one. There are significantly more left-billed crossbills than right-billed.

### Extrinsic Hypothesis for Rice Herbicides

Shivrain et al. (2006) crossed clearfield rice, which are resistant to the herbicide imazethapyr, with red rice, which are susceptible to imazethapyr. They then crossed the hybrid offspring and examined the F2 generation, where they found 772 resistant plants, 1611 moderately resistant plants, and 737 susceptible plants. If resistance is controlled by a single gene with two co-dominant alleles, you would expect a 1:2:1 ratio. Compare the observed numbers with the 1:2:1 ratio using `chisq.test`.

```{r rice, exercise = TRUE}

```
```{r rice-solution}
observe <- c(772, 1611, 737)
expected <- c(0.25, 0.50, 0.25)

chisq.test(x = observed,
           p = expected)
```

### Extrinsic Hypothesis for Bird Foraging Behavior

Mannan and Meslow (1984) studied bird foraging behavior in a forest in Oregon. In a managed forest, 54% of the canopy volume was Douglas fir, 40% was ponderosa pine, 5% was grand fir, and 1% was western larch. They made 156 observations of foraging by red-breasted nuthatches; 70 observations (45% of the total) in Douglas fir, 79 (51%) in ponderosa pine, 3 (2%) in grand fir, and 4 (3%) in western larch. The biological null hypothesis is that the birds forage randomly, without regard to what species of tree they're in; the statistical null hypothesis is that the proportions of foraging events are equal to the proportions of canopy volume. Set this up and perform a `chisq.test`.

```{r bird-foraging, exercise = TRUE}

```
```{r bird-foraging-solution}
observed <- c(70, 79, 3, 4)
expected <- c(0.54, 0.40, 0.05, 0.01)

chisq.test(x = observed,
           p = expected)
```

Notice that the code produced a warning. The expected numbers in this example are pretty small for some groups, so it would be better to analyze it with an exact test. It's here because it's a good example of an extrinsic hypothesis that comes from measuring something (canopy volume, in this case) rather than from a mathematical theory. Those sorts of examples can be hard to find. 

### Intrinsic Hypothesis for Alleles

McDonald (1989) examined variation at the Mpi locus in the amphipod crustacean Platorchestia platensis collected from a single location on Long Island, New York. There were two alleles, Mpi<sup>90</sup> and Mpi<sup>100</sup>, and the genotype frequencies in samples from multiple dates pooled together were 1203 Mpi<sup>90/90</sup>, 2919 Mpi<sup>90/100</sup>, and 1678 Mpi<sup>100/100</sup>. The estimate of the Mpi<sup>90</sup> allele proportion from the data is 5325/11600 = 0.459. 

Using the Hardy-Weinberg formula and this estimated allele proportion, the expected genotype proportions are 0.211 Mpi<sup>90/90</sup>, 0.497 Mpi<sup>90/100</sup>, and 0.293 Mpi<sup>100/100</sup>. There are three categories (the three genotypes) and one parameter estimated from the data (the Mpi<sup>90</sup> allele proportion), so there is one degree of freedom. 

You might be tempted to run this analysis as you did the others:

>observed <- c(1203, 2919, 1678)    
expected < c(.211, .497, .293)    
chisq.test(observed, expected)    

You would get these results:

>    Pearson's Chi-squared test    
data:  observed and expected_prop    
X-squared = 6, df = 4, p-value = 0.1991,     

which would be wrong. 

```{r why_not_chisq, echo=FALSE}
question("Why?",
         answer("It's not wrong, it's right."),
         answer("Because there is a typo in the expected proportions"),
         answer("Because the degrees of freedom should number 3 to account for the fact that one calculation is already done before running `chisq.test`."),
         answer("`chisq.test` assumes that the degrees of freedom in this case number 2 (*k* = 2 groups = 2 df). In fact they number 1 (*k* = 2 groups - 1 df for the calculation = 1 df), because a degree of freedom was eliminated by calculating the expected proportions based on the observed allele counts", correct = TRUE)
)
```

We can't use `chisq.test` because of its faulty assumption about the degrees of freedom. Let's calculate a variable `chi2` manually by plugging the correct values into the formula noted above and repeated here for your convenience:

$\displaystyle \chi^{2} = ∑\frac{(O−E)^{2}}{E}$,

where O = observed and E = expected. 

It looks complicated, but it isn't. It's a fairly simple series of statements in R. You will need 3 variables: `observed`, a concatenation of the 3 previously mentioned observed counts; `expected_prop`, a concatenation of the 3 previously mentioned expected proportions, and `expected_count`, the sum of the observations multiplied by `expected_prop`. 

Have set up those three values, you can now calculate `chi2` to be the sum of the quantity `observed - expected_count` (which is what `∑(O - E)` means), square it, then that quantity divided by the expected count. To get you started, this is the sum of the differences between `observed` and `expected` squared:

>sum((observed - expected_count)^2).

Divide that by the expected count and you have your `chi2`. 

Try it now, and if you get stuck, peek at the solution by clicking on the Solution button at the top of the code chunk.


```{r prepare-alleles}
observed <- c(1203,  2919,  1678)
expected_prop <- c(0.211, 0.497, 0.293)
expected_count <- sum(observed) * expected_prop

chi2 <- sum((observed - expected_count)^2 / expected_count)
```
```{r alleles, exercise=TRUE}

```
```{r alleles-solution}
observed <- c(1203,  2919,  1678)
expected_prop <- c(0.211, 0.497, 0.293)
expected_count <- sum(observed) * expected_prop

chi2 <- sum((observed - expected_count)^2 / expected_count)
```

That supplies `chi2`, but it does not tell us what the odds are of having a distribution shaped like this (or one more extreme) by chance. Fortunately (and of course) someone has already written a function that does this and you can use it. Call `pchisq`, passing to it the value for `chi2` you calculated in the last code chunk, the degrees of freedom (remember that this is 1 and not 2), and `lower.tail` set equal to `FALSE`. We don't need to know, right now, what `lower.tail` is all about.

```{r pchisq, exercise = TRUE, exercise.setup = "prepare-alleles"}

```
```{r pchisq-solution}
P <- pchisq(chi2,
       df = 1,
       lower.tail = FALSE) 
P
```

The result is not significant. You cannot reject the null hypothesis that the data fit the expected Hardy-Weinberg proportions. 

>Memorize this forever: *You haven't proven the null*. You can never prove a null by deciding you can't with confidence reject it. The bar is very low for proving you can't reject a null hypothesis. It must be much higher before you can with confidence utterly and forever reject the alternative hypothesis and claim the null hypothesis is true.

## Graphing the Results

If there are just two values of the nominal variable, you shouldn't display the result in a graph, as that would be a bar graph with just one bar. Instead, just report the proportion; for example, Groth (1992) found 52.0% left-billed crossbills.

With more than two values of the nominal variable, you should usually present the results of a goodness-of-fit test in a table of observed and expected proportions. If the expected values are obvious (such as 50%) or easily calculated from the data (such as Hardy–Weinberg proportions), you can omit the expected numbers from your table. For a presentation you'll probably want a graph showing both the observed and expected proportions, to give a visual impression of how far apart they are. You should use a bar graph for the observed proportions; the expected can be shown with a horizontal dashed line, or with bars of a different pattern.

If you want to add error bars to the graph, you should use confidence intervals for a proportion. Note that the confidence intervals will not be symmetrical, and this will be particularly obvious if the proportion is near 0 or 1.

Some people use a "stacked bar graph" to show proportions, especially if there are more than two categories. However, it can make it difficult to compare the sizes of the observed and expected values for the middle categories, since both their tops and bottoms are at different levels, so I don't recommend it.

### Simple Bar Plot with `barplot`

First, set up the input data. This is very similar to what we have done before, but we will also use raw data thatI have  loaded for you in a matrix called `trees`. Our observed and expected values are the same as before:

>observed_trees <- c(70, 79, 3, 4)    
expected_trees <- c(0.54, 0.40, 0.05, 0.01)    

1. Create the variables `observed_trees`, `expected_trees`, and `total_trees` (`sum` the `observed_trees` values).      
2. Make `observed_trees_prop` the proportion of `observed_trees` to `total`, or `observed_trees / total`.      
3. Look at what is in `observed_trees_prop`.     
4. Look at what is the in previously-loaded matrix `trees`.     
5. Create the bar plot using `barplot`. Pass it `trees`, set `beside` equal to `TRUE`, `legend` equal to `TRUE`, and then set limits on which y values will be included in the bar plot by making `ylim` equal to the concatenation of 0.0 and 0.6. Finally, add labels by making `xlab` equal to "Tree Species" and `ylab` equal to "Foraging Proportion".      
6. Run the code chunk.    

```{r trees, exercise = TRUE}


```
```{r trees-solution}
observed_trees <- c(70, 79, 3, 4)
expected_trees <- c(0.54, 0.40, 0.05, 0.01)
total <- sum(observed_trees)
observed_trees_prop <- observed_trees / total

observed_trees_prop
trees

barplot(height = trees,
        beside = TRUE,
        legend = TRUE,
        ylim = c(0.0, 0.6),
        xlab = "Tree Species",
        ylab = "Foraging Proportion"
        )
```

### Bar Plot with Confidence Intervals with `ggplot2`

Now try a bar chart with confidence intervals. You'll need some code to calculate the confidence intervals. Sometimes factors will need to have the order of their levels specified for `ggplot2` to put them in the correct order in the plot. Otherwise R will alphabetize them.

I have already loaded the data in `forage`.

Here are the steps to follow to print the contents of `forage` to accompany a chi-square goodness-of-fit test:

1. Specify the order of factor levels (unless you want R to alphabetize them) using `dplyr`    
2. Calculate the confidence intervals. 

The following code performs those steps and shows you how `forage` changes. Experiment with the code if you like, but then copy the solution and run it again to make sure all the values remain as we need them for subsequent code chunks. 

```{r forage, exercise = TRUE}
library(dplyr)
forage <- mutate(forage,
                  Tree = factor(Tree, levels = unique(Tree)),
                  Value = factor(Value, levels = unique(Value))
                  )
forage

forage <- 
mutate(forage,       
       low.ci = apply(forage[c("Count", "Total", "Expected")], 
                       1, 
                       function(x) 
                       binom.test(x["Count"], x["Total"], x["Expected"]
                                 )$conf.int[1]),
                                 
        upper.ci = apply(forage[c("Count", "Total", "Expected")],
                         1,
                         function(x) 
                         binom.test(x["Count"], x["Total"], x["Expected"]
                                    )$conf.int[2])
         )

forage$low.ci[forage$ Value == "Expected"] = 0
forage$upper.ci[forage$ Value == "Expected"] = 0

forage
```
```{r forage-solution}
library(dplyr)
forage <- mutate(forage,
                  Tree = factor(Tree, levels = unique(Tree)),
                  Value = factor(Value, levels = unique(Value))
                  )
forage = 
mutate(forage,       
       low.ci = apply(forage[c("Count", "Total", "Expected")], 
                       1, 
                       function(x) 
                       binom.test(x["Count"], x["Total"], x["Expected"]
                                 )$conf.int[1]),
                                 
        upper.ci = apply(forage[c("Count", "Total", "Expected")],
                         1,
                         function(x) 
                         binom.test(x["Count"], x["Total"], x["Expected"]
                                    )$conf.int[2])
         )

forage$low.ci[forage$ Value == "Expected"] = 0
forage$upper.ci[forage$ Value == "Expected"] = 0

forage
```

Now we plot it. For more information on `ggplot2` see [the documentation at the tidyverse website](https://ggplot2.tidyverse.org). 

If you are familiar with `ggplot2` you can build the plot. The data is, of course, `forage`. For aesthetics (`aes`), `x` is the variable `Tree` and `y` is `Proportion` (which we just now created). Use `Value` for the fill.

The geom we want is `geom_bar`. In fact we want 2 of them. They both have arguments `stat = "identity"` and `position = "dodge"`; the first one has a width of 0.7. The second `geom_bar` has `colour` "black", `width` 0.7, and `show.legend` `FALSE`. 

Next add the function `scale_y_continues`, with breaks at `seq(0, 0.60, 0.1), `limits` `c(0, 0.60)`, and `expand` `c(0,0)`.

Next add another function, `scale_fill_manual`, with arguments `name` equal to "Count Type", `values` equal to `c('grey80', 'grey30'), and `labels` as the concatenation of "Observed Value" and "Expected Value". 

So much for the basic chart and its bars. Now add the error bars. Use the Solution button to see what the code is for them and for the title, unless you want to figure out it before looking at my code. Well, not my code. It is adapted from the [R Graph Catalogue](http://shinyapps.stat.ubc.ca/r-graph-catalog/), which you might want to explore. 

Once you have all the code in your chunk, execute it. 

```{r ggplot-trees, exercise = TRUE}

```
```{r ggplot-trees-solution}
library(ggplot2)
library(grid)

ggplot(forage, 
   aes(x = Tree, y = Proportion, fill = Value, 
       ymax = upper.ci, ymin = low.ci))  +
       geom_bar(stat = "identity", position = "dodge", width = 0.7) +
       geom_bar(stat = "identity", position = "dodge", 
                colour = "black", width = 0.7, 
                show.legend = FALSE)  +
       scale_y_continuous(breaks = seq(0, 0.60, 0.1),
                limits = c(0, 0.60), 
                expand = c(0, 0))  +
       scale_fill_manual(name = "Count type" , 
                 values = c('grey80', 'grey30'), 
                 labels = c("Observed value", 
                            "Expected value"))  +
       geom_errorbar(position=position_dodge(width=e0.7), 
                     width=0.0, size=0.5, color="black")  +
       labs(x = "Tree species", 
            y = "Foraging proportion")  +
       ## ggtitle("Main title") + 
       theme_bw()  +
       theme(panel.grid.major.x = element_blank(),
             panel.grid.major.y = element_line(colour = "grey50"),
             plot.title = element_text(size = rel(1.5), 
             face = "bold", vjust = 1.5),
             axis.title = element_text(face = "bold"),
             legend.position = "top",
             legend.title = element_blank(),
             legend.key.size = unit(0.4, "cm"),
             legend.key = element_rect(fill = "black"),
             axis.title.y = element_text(vjust= 1.8),
             axis.title.x = element_text(vjust= -0.5)
            )
```

## Power Analysis

Each statistical methodology has slightly different calculations for power. 

### Using G*Power

To do a power analysis using the G*Power program, choose "Goodness-of-fit tests: Contingency tables" from the Statistical Test menu, then choose "Chi-squared tests" from the Test Family menu. To calculate effect size, click on the Determine button and enter the null hypothesis proportions in the first column and the proportions you hope to see in the second column. Then click on the Calculate and Transfer to Main Window button. Set your alpha and power, and be sure to set the degrees of freedom (Df); for an extrinsic null hypothesis, that will be the number of rows minus one.

### Using R's `pwr` Function

As an example of calculating power in R, let's say you want to do a genetic cross of snapdragons with an expected 1:2:1 ratio, and you want to be able to detect a pattern with 5% more heterozygotes that expected. Store 0.25, 0.50, and 0.25 in the variable `p0` and 0.225, 0.55, and 0.225 in `p1`. Make sure you have a statement `library(pwr)`. Then use the function `ES.w1` and pass it the two variables `p0` and `p1`. Create a variable `degrees` and make it the length of `p0` minus 1. 

Next use the function `pwr.chisq.test` and pass it `w = effect.size`, `N = NULL`, `df = degrees`, `power = 0.80`, and `sig.level = 0.05`. 

Run your code chunk.

```{r chisq-power, exercise = TRUE}

```
```{r chisq-power-solution}
library(pwr)

P0      = c(0.25,  0.50, 0.25)
P1      = c(0.225, 0.55, 0.225) 

effect.size = ES.w1(P0, P1)  

degrees = length(P0) - 1

pwr.chisq.test(
               w=effect.size, 
               N=NULL,            # Total number of observations, to be calculated
               df=degrees, 
               power=0.80,        # 1 minus Type II probability
               sig.level=0.05)    # Type I probability
```

If you got `N = 963.4689` or thereabouts, you did it right!

## References

John H. McDonald's [Handbook of Biological Statistics](http://www.biostathandbook.com/kruskalwallis.html)

Salvatore S. Mangiafico's [An R Companion for the Handbook of Biostatistics](https://rcompanion.org/rcompanion/a_02.html)